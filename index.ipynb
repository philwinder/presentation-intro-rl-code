{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "201102_code_driven_rl_intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPVuaH74b4AGCSGrdQ9ftVo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/philwinder/presentation-intro-rl-code/blob/main/index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjuBuBeVi6pE"
      },
      "source": [
        "# Code-Driven Introduction to Reinforcement Learning\n",
        "\n",
        "Welcome, this is an example from the book [Reinforcement Learning](https://rl-book.com/?utm_source=winder&utm_medium=notebook&utm_campaign=rl), by Dr. Phil Winder and this notebook is from [the rl-book.com website](https://rl-book.com/learn/mdp/code_driven_intro/?utm_source=winder&utm_medium=notebook&utm_campaign=rl).\n",
        "\n",
        "In this notebook you will be investigating the fundamentals of reinforcement learning (RL). The first section describes the Markov decision process (MDP), which is a framework to help you design problems. The second section formulates an RL-driven solution for the MDP.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "This notebook was developed to work in [Binder](https://mybinder.org/) or [Google's colabratory](https://colab.research.google.com/) -- other notebook hosts are available. This allows you to launch your own notebook and tinker around with the code. I expect that you are familiar with a Jupyter notebook.\n",
        "\n",
        "I also expect that you are familiar with Python, since the code is written in this language.\n",
        "\n",
        "### Expectations\n",
        "\n",
        "In the following examples I intentionally use a very simple and visual example. This makes it easier to understand, since that is the main goal. Although there are [industrial examples](https://rl-book.com/applications/?utm_source=winder&utm_medium=notebook&utm_campaign=rl) that are similar, real-life implementations are likely to be more complex.\n",
        "\n",
        "## Section 1. The Markov Decision Process\n",
        "\n",
        "The Markov decision process (MDP) is a mathematical framework that helps you encapsulate the real-world. Desptite simple and restrictive -- the sign of a good interface -- a suprising number of situations can be squeezed into the MDP formalism.\n",
        "\n",
        "### The MDP Entities\n",
        "\n",
        "An MDP has two \"entities\":\n",
        "\n",
        "- **An agent**: An application that is able to observe state and suggest an action. It also receives feedback to let it know whether the action was good, or not.\n",
        "- **An environment**: This is the place where the agent acts within. It accepts an action, which alterts its internal state, and produces a new observation of that state.\n",
        "\n",
        "In nearly all of the examples that I have seen, these two entities are implemented independently. The agent is often an RL algorithm (although not always) and the environment is either real life or a simulation.\n",
        "\n",
        "### The MDP Interface\n",
        "\n",
        "The agent and the environment interact through an interface. You have some control over what goes into that interface and a large amount of effort is typically spent improving the quality of the data that flows through it. You need representations of:\n",
        "\n",
        "- **State**: This is the observation of the environment. You often get to choose what to \"show\" to the agent. There is a compromise between simplifying the state to speed up learning and preventing overfitting, but often it pays to include as much as you can.\n",
        "- **Action**: Your agent must suggest an action. This mutates the environment in some way. So called \"options\" or \"null-actions\" allow you to do nothing, if that's what you want to do.\n",
        "- **Reward**: You use the reward to fine-tune your action choices.\n",
        "\n",
        "### Creating a \"GridWorld\" Environment\n",
        "\n",
        "To make it easy to understand, I'm going to show you how to create a simulation of a simple grid-based \"world\". Many real-life implementations begin with a simulation of the real world, because it's much easier to iterate and improve your design with a software stub of real-life.\n",
        "\n",
        "The goal of this environment is to define a \"world\" in which a \"robot\" can move. The so-called-world is actually a series of cells inside a 2-dimensional box. The agent can move north, east, south, or west which moves the robot between the cells. The goal of the environment is to reach a goal. There is a reward of -1 for every step, to promote reaching the goal as fast as possible.\n",
        "\n",
        "#### Imports and Definitions\n",
        "\n",
        "First let me import a few libraries (to enable the autocompletion in later cells) and define a few important definitions. The first is the defacto definition of a \"point\" object, with x and y coordinates and the second is a direction enumeration. These are use to define the position of the agent in the environment and the direction of movement for an action, respectively. Note that I'm assuming that east moves in a positive x direction and north moves in a positive y direction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ38NOIkwCi0"
      },
      "source": [
        "from collections import defaultdict, namedtuple\n",
        "from enum import Enum\n",
        "from typing import Tuple, List\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "Point = namedtuple('Point', ['x', 'y'])\n",
        "class Direction(Enum):\n",
        "  NORTH = \"⬆\"\n",
        "  EAST = \"⮕\"\n",
        "  SOUTH = \"⬇\"\n",
        "  WEST = \"⬅\"\n",
        "\n",
        "  @classmethod\n",
        "  def values(self):\n",
        "    return [v for v in self]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV_vGVIaiZPz"
      },
      "source": [
        "#### The Environment Class\n",
        "\n",
        "Next I create a Python class that represents the environment. The first function in the class is the initialisation function in which we can specify the width and height of the environment.\n",
        "\n",
        "After that I define a helper parameter which encodes the possible actions and then I reset the state of the environment with a `reset` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k64kfJi3jBaq"
      },
      "source": [
        "class SimpleGridWorld(object):\n",
        "  def __init__(self, width: int = 5, height: int = 5, debug: bool = False):\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.debug = debug\n",
        "    self.action_space = [d for d in Direction]\n",
        "    self.reset()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHXiX7RAjEdf"
      },
      "source": [
        "#### The Reset Function\n",
        "\n",
        "Many environments have an implicit \"reset\", whereby the environment's state is moved away from the goal state. In this implementation I reset the environment back to the `(0, 0)` position, but this isn't strictly necessary. Many real-life environments reset randomly or have no reset at all.\n",
        "\n",
        "Here I also set the goal, which is located in the south-eastern corner of the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkPQCnrRjqIO"
      },
      "source": [
        "class SimpleGridWorld(SimpleGridWorld):\n",
        "  def reset(self):\n",
        "    self.cur_pos = Point(x=0, y=(self.height - 1))\n",
        "    self.goal = Point(x=(self.width - 1), y=0)\n",
        "    # If debug, print state\n",
        "    if self.debug:\n",
        "      print(self)\n",
        "    return self.cur_pos, 0, False"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f70YnYwGjtwV"
      },
      "source": [
        "#### Taking a Step\n",
        "\n",
        "Recall that the MDP interface three key components: the state, the action, and the reward. The environment's step function accepts an action, then produces a new state and reward.\n",
        "\n",
        "The large amount of code is a consequence of the direction implementation. You can refactor this to use fewer lines of code with some clever indexing. However, I think this level of verbosity helps explain what is going on. In essence, every direction moves the current position by one square. You can see the code incrementing or decrementing the x or y coordinates.\n",
        "\n",
        "The second part of the function is testing to see if the agent is at the goal. If it is, then it signals that it is at a terminal state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rCJNuVnjtA2"
      },
      "source": [
        "class SimpleGridWorld(SimpleGridWorld):\n",
        "  def step(self, action: Direction):\n",
        "    # Depending on the action, mutate the environment state\n",
        "    if action == Direction.NORTH:\n",
        "      self.cur_pos = Point(self.cur_pos.x, self.cur_pos.y + 1)\n",
        "    elif action == Direction.EAST:\n",
        "      self.cur_pos = Point(self.cur_pos.x + 1, self.cur_pos.y)\n",
        "    elif action == Direction.SOUTH:\n",
        "      self.cur_pos = Point(self.cur_pos.x, self.cur_pos.y - 1)\n",
        "    elif action == Direction.WEST:\n",
        "      self.cur_pos = Point(self.cur_pos.x - 1, self.cur_pos.y)\n",
        "    # Check if out of bounds\n",
        "    if self.cur_pos.x >= self.width:\n",
        "      self.cur_pos = Point(self.width - 1, self.cur_pos.y)\n",
        "    if self.cur_pos.y >= self.height:\n",
        "      self.cur_pos = Point(self.width, self.cur_pos.y - 1)\n",
        "    if self.cur_pos.x < 0:\n",
        "      self.cur_pos = Point(0, self.cur_pos.y)\n",
        "    if self.cur_pos.y < 0:\n",
        "      self.cur_pos = Point(self.cur_pos.x, 0)\n",
        "\n",
        "    # If at goal, terminate\n",
        "    is_terminal = self.cur_pos == self.goal\n",
        "\n",
        "    # Constant -1 reward to promote speed-to-goal\n",
        "    reward = -1\n",
        "\n",
        "    # If debug, print state\n",
        "    if self.debug:\n",
        "      print(self)\n",
        "\n",
        "    return self.cur_pos, reward, is_terminal"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDHb8l_-lJKn"
      },
      "source": [
        "#### Visualisation\n",
        "\n",
        "And finally, like all of data science, it is vitally important that you are able to visualise the behaviour and performance of your agent. The first step in this process is being able to visualise the agent within the environment. The next function does this by printing a textual grid, with an `x` at the agent's location, a `o` at the goal, an `@` if the agent is on top of the goal, and a `_` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGOg2vh_sNEA"
      },
      "source": [
        "class SimpleGridWorld(SimpleGridWorld):\n",
        "  def __repr__(self):\n",
        "    res = \"\"\n",
        "    for y in reversed(range(self.height)):\n",
        "      for x in range(self.width):\n",
        "        if self.goal.x == x and self.goal.y == y:\n",
        "          if self.cur_pos.x == x and self.cur_pos.y == y:\n",
        "            res += \"@\"\n",
        "          else:\n",
        "            res += \"o\"\n",
        "          continue\n",
        "        if self.cur_pos.x == x and self.cur_pos.y == y:\n",
        "          res += \"x\"\n",
        "        else:\n",
        "          res += \"_\"\n",
        "      res += \"\\n\"\n",
        "    return res"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w3o6SH6lykl"
      },
      "source": [
        "### Running the Environment\n",
        "\n",
        "To run the environment you need to instantiate the class, call reset to move the agent back to the start, then perform a series of actions to move the agent. For now let me move it manually, to make sure it is working, visualising the agent at each step. I also print the result of the step (the new state, reward, and terminal flag) for completeness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-IuuVBjv_tc",
        "outputId": "03161fe6-1f82-456f-85f5-131474290516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "s = SimpleGridWorld(debug=True)\n",
        "print(\"☝ This shows a simple visualisation of the environment state.\\n\")\n",
        "s.step(Direction.SOUTH)\n",
        "print(s.step(Direction.SOUTH), \"⬅ This displays the state and reward from the environment 𝐀𝐅𝐓𝐄𝐑 moving.\\n\")\n",
        "s.step(Direction.SOUTH)\n",
        "s.step(Direction.SOUTH)\n",
        "s.step(Direction.EAST)\n",
        "s.step(Direction.EAST)\n",
        "s.step(Direction.EAST)\n",
        "s.step(Direction.EAST)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x____\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "____o\n",
            "\n",
            "☝ This shows a simple visualisation of the environment state.\n",
            "\n",
            "_____\n",
            "x____\n",
            "_____\n",
            "_____\n",
            "____o\n",
            "\n",
            "_____\n",
            "_____\n",
            "x____\n",
            "_____\n",
            "____o\n",
            "\n",
            "(Point(x=0, y=2), -1, False) ⬅ This displays the state and reward from the environment 𝐀𝐅𝐓𝐄𝐑 moving.\n",
            "\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "x____\n",
            "____o\n",
            "\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "x___o\n",
            "\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "_x__o\n",
            "\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "__x_o\n",
            "\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "___xo\n",
            "\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "____@\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Point(x=4, y=0), -1, True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwvpSbUwngL-"
      },
      "source": [
        "#### Key Takeaways\n",
        "\n",
        "There are a few key lessons that you should commit to memory:\n",
        "\n",
        "- The **state** is an observation of the environment, which contains everything outside of the agent. For example, the agent's current position within the environment. In real world applications this could be the time of the day, the weather, data from a video camera, literally anything.\n",
        "- The **reward** fully specifies the optimal solution to the problem. In real life this might be profit or the number of new customers.\n",
        "- Every **action** mutates the state of the environment. This may or may not be observable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y74EZUoSo60h"
      },
      "source": [
        "## A Reinforcement Learning Solution to the MDP: Implementing the Monte Carlo RL Algorithm\n",
        "\n",
        "Rather confusingly, RL, like ML, is meant both as a technique and a collection of algorithms. Exactly _when_ an algorithm becomes an RL algorithms is up for debate, but it is generally accepted that there has to be multiple steps (otherwise it would just be a bandit problem) and it attempts to quantify the value of being in a particular state.\n",
        "\n",
        "An algorithm called the _cross-entropy method_ is an algorithm that attempts to stumble accross the goal. However, once it has then it replicates the same movements again to reach the goal. This is not stricly learning, it is memoising, so it is not an RL algorithm. However, you shouldn't discount it, because it is a very good and simple baseline. It can easily complete very sophisticated tasks if you give it enough time.\n",
        "\n",
        "Instead, let me introduce a slight variation to this algorithm called the Monte Carlo (MC) method. This lies at the heart of all modern RL algorithms so it is a great way to start.\n",
        "\n",
        "In short -- you can read more about this in Chapter 2 of the book -- MC methods attempt to randomly sample states and judge their value. Once you have sampled the states enough times then you can derive a strategy that follows the path of the next best value.\n",
        "\n",
        "Let's give it a try.\n",
        "\n",
        "### Generating trajectories\n",
        "\n",
        "Monte Carlo techniques operate by sampling the environment. In general, the idea is that if you can sample the environment enough times, you can begin to build a picture of the output, given any input. We can use this idea in RL. If we capture enough _trajectories_, where a trajectory is one full pass through an environment, then we can see which states are advantagous.\n",
        "\n",
        "To begin, I will create a class that is capable of generating trajectories. Here I pass in the environment, then in the `run` function I repeatedly step in the environment using a random action. I store each step in a list and return it to the user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34Y8V-k89dFS"
      },
      "source": [
        "class MonteCarloGeneration(object):\n",
        "  def __init__(self, env: object, max_steps: int = 1000, debug: bool = False):\n",
        "    self.env = env\n",
        "    self.max_steps = max_steps\n",
        "    self.debug = debug\n",
        "\n",
        "  def run(self) -> List:\n",
        "    buffer = []\n",
        "    n_steps = 0 # Keep track of the number of steps so I can bail out if it takes too long\n",
        "    state, _, _ = self.env.reset() # Reset environment back to start\n",
        "    terminal = False\n",
        "    while not terminal: # Run until terminal state\n",
        "      action = random.choice(self.env.action_space) # Random action. Try replacing this with Direction.EAST\n",
        "      next_state, reward, terminal = self.env.step(action) # Take action in environment\n",
        "      buffer.append((state, action, reward)) # Store the result\n",
        "      state = next_state # Ready for the next step\n",
        "      n_steps += 1\n",
        "      if n_steps >= self.max_steps:\n",
        "        if self.debug:\n",
        "          print(\"Terminated early due to large number of steps\")\n",
        "        terminal = True # Bail out if we've been working for too long\n",
        "    return buffer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly8Zmy5J-NFu"
      },
      "source": [
        "#### Visualising Trajectories\n",
        "\n",
        "As before, it's vitally important to visualise as much as possible, to gain an intuition into your problem. A simple first step is to view the agent's movement and trajectory. Here I severely limit the amount of exploration to save reams of output. Depending on your random seed you will see the agent stumbling around."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymk3yN7H_AEU",
        "outputId": "036410de-2292-4806-a923-3b87ac85fcc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = SimpleGridWorld(debug=True) # Instantiate the environment\n",
        "generator = MonteCarloGeneration(env=env, max_steps=10, debug=True) # Instantiate the generation\n",
        "trajectory = generator.run() # Generate a trajectory\n",
        "print([t[1].value for t in trajectory]) # Print chosen actions\n",
        "print(f\"total reward: {sum([t[2] for t in trajectory])}\") # Print final reward"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x____\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "____o\n",
            "\n",
            "x____\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "____o\n",
            "\n",
            "_____\n",
            "x____\n",
            "_____\n",
            "_____\n",
            "____o\n",
            "\n",
            "x____\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "____o\n",
            "\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "____o\n",
            "\n",
            "____x\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "____o\n",
            "\n",
            "___x_\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "____o\n",
            "\n",
            "_____\n",
            "___x_\n",
            "_____\n",
            "_____\n",
            "____o\n",
            "\n",
            "_____\n",
            "____x\n",
            "_____\n",
            "_____\n",
            "____o\n",
            "\n",
            "_____\n",
            "_____\n",
            "____x\n",
            "_____\n",
            "____o\n",
            "\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "____x\n",
            "____o\n",
            "\n",
            "_____\n",
            "_____\n",
            "_____\n",
            "____x\n",
            "____o\n",
            "\n",
            "Terminated early due to large number of steps\n",
            "['⬇', '⬆', '⬆', '⬅', '⬅', '⬇', '⮕', '⬇', '⬇', '⮕']\n",
            "total reward: -10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2yDx2cnAS3G"
      },
      "source": [
        "### Quantifying Value\n",
        "\n",
        "There's an important quanity called the _action value function_. In summary, it is a measure of the value of taking a particular action, given all the experience. In other words, you can look at the previous trajectories, find out which of them lead to the highest values and look to use them again. See Chapter 2 in the book for more details.\n",
        "\n",
        "To generate an estimate of this value, generate a full trajectory, then look at how far away the agent is from the terminal states at all steps.\n",
        "\n",
        "So this means we need a class to generate a full trajectory, from start to termination. That code is below. First I create a new class that accepts the generator from before; I'll use this later to generate the full trajectory.\n",
        "\n",
        "Then I create two fields to retain the experience observed by the agent. The first is recording the expected value at each state. This is the effectively the distance to the goal. The second is recording the number of times the agent has visited that state.\n",
        "\n",
        "Then I create a helper function to return a key for the dictionary (a.k.a. map) and an action value function to calculate the value of taking each action in each state. This is simply the average value over all visits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CSqugUJyR_3"
      },
      "source": [
        "class MonteCarloExperiment(object):\n",
        "  def __init__(self, generator: MonteCarloGeneration):\n",
        "    self.generator = generator\n",
        "    self.values = defaultdict(float)\n",
        "    self.counts = defaultdict(float)\n",
        "\n",
        "  def _to_key(self, state, action):\n",
        "    return (state, action)\n",
        "  \n",
        "  def action_value(self, state, action) -> float:\n",
        "    key = self._to_key(state, action)\n",
        "    if self.counts[key] > 0:\n",
        "      return self.values[key] / self.counts[key]\n",
        "    else:\n",
        "      return 0.0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWH_Gxw6BVrW"
      },
      "source": [
        "Next I create a function to store this data after generating a full trajectory. There are several important parts of this function.\n",
        "\n",
        "The first is that I'm using reversed trajectories. I.e. I'm starting from the end and working backwards.\n",
        "\n",
        "The second is that I'm averaging the expected return over all visits. So this is reporting the value of an action, on average."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDXn0UmFBU6Z"
      },
      "source": [
        "class MonteCarloExperiment(MonteCarloExperiment):\n",
        "  def run_episode(self) -> None:\n",
        "    trajectory = self.generator.run() # Generate a trajectory\n",
        "    episode_reward = 0\n",
        "    for i, t in enumerate(reversed(trajectory)): # Starting from the terminal state\n",
        "      state, action, reward = t\n",
        "      key = self._to_key(state, action)\n",
        "      episode_reward += reward  # Add the reward to the buffer\n",
        "      self.values[key] += episode_reward # And add this to the value of this action\n",
        "      self.counts[key] += 1 # Increment counter"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAFFF6ZhBUmZ"
      },
      "source": [
        "#### Running the Trajectory Generation\n",
        "\n",
        "Let's test this by setting some expectations. We're reporting the value of taking an action **on average**. So on average, you would expect the value of taking the `EAST` action when next to the terminal state would be -1, because it's right there, it's a single step and therefore a single reward of -1 to get to the terminal state.\n",
        "\n",
        "However, other directions will not be -1, because the agent will continue to stumble around."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lz4m1mgpreTy",
        "outputId": "a2e69bbb-862e-48f6-cdd5-e2746d13a1a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = SimpleGridWorld(debug=False) # Instantiate the environment - set the debug to true to see the actual movemen of the agent.\n",
        "generator = MonteCarloGeneration(env=env, debug=True) # Instantiate the trajectory generator\n",
        "agent = MonteCarloExperiment(generator=generator)\n",
        "for i in range(4):\n",
        "  agent.run_episode()\n",
        "  print(f\"Run {i}: \", [agent.action_value(Point(3,0), d) for d in env.action_space])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run 0:  [0.0, -1.0, 0.0, -8.0]\n",
            "Run 1:  [0.0, -1.0, 0.0, -8.0]\n",
            "Run 2:  [-7.0, -1.0, 0.0, -5.5]\n",
            "Run 3:  [-13.0, -1.0, -14.0, -5.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omKjnso-DdVW"
      },
      "source": [
        "So you can see from above that yes, when choosing east from the point to the west of the terminal state the expected return is -1. But notice that the agent (probably) did not observe that result straight away, because it takes time to randomly select it. (Run it a few more times to see what happens, you'll see random changes)\n",
        "\n",
        "#### Visualising the State Value Function\n",
        "\n",
        "The state value function is the average expected return for all actions. In general, you should see that the value increases the closer you get to the goal. But because of the random movement, especially far away from the goal, there will be a lot of noise.\n",
        "\n",
        "Below I create a helper function to plot this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAkaVRETrt65",
        "outputId": "477d3a9e-2b27-405a-ba70-e5d0885c6a2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def state_value_2d(env, agent):\n",
        "    res = \"\"\n",
        "    for y in reversed(range(env.height)):\n",
        "      for x in range(env.width):\n",
        "        if env.goal.x == x and env.goal.y == y:\n",
        "          res += \"   @  \"\n",
        "        else:\n",
        "          state_value = sum([agent.action_value(Point(x,y), d) for d in env.action_space]) / len(env.action_space)\n",
        "          res += f'{state_value:6.2f}'\n",
        "        res += \" | \"\n",
        "      res += \"\\n\"\n",
        "    return res\n",
        "\n",
        "print(state_value_2d(env, agent))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-44.73 | -11.50 |  -5.00 | -25.75 | -30.49 | \n",
            "-40.58 | -22.88 | -15.25 | -34.71 | -27.36 | \n",
            "-12.67 | -10.67 | -10.50 | -22.50 | -19.44 | \n",
            "-14.79 | -13.38 | -15.00 | -20.40 | -17.38 | \n",
            "-19.34 | -18.50 | -13.62 |  -8.38 |    @   | \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "armEvkb8EXux"
      },
      "source": [
        "### Generating Optimal Policies\n",
        "\n",
        "A _policy_ is a set of rules that an agent should follow. It is a strategy that works for that particular environment. You can now generate thousands of trajectories and track the expected value over time.\n",
        "\n",
        "With enough averaging, the expected values should present a clear picture of what the optimal policy is. See if you can see what it is?\n",
        "\n",
        "In the code below I'm instantiating all the previous code and then generating 1000 episodes. Then I print out the state value function for every position."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "434JeYohWxUK",
        "outputId": "9f36603d-f7ce-4f73-be4a-4efc759d35a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = SimpleGridWorld() # Instantiate the environment\n",
        "generator = MonteCarloGeneration(env=env) # Instantiate the trajectory generator\n",
        "agent = MonteCarloExperiment(generator=generator)\n",
        "for i in range(1000):\n",
        "  clear_output(wait=True)\n",
        "  agent.run_episode()\n",
        "  print(f\"Iteration: {i}\")\n",
        "  # print([agent.action_value(Point(0,4), d) for d in env.action_space]) # Uncomment this line to see the actual values for a particular state\n",
        "  print(state_value_2d(env, agent), flush=True)\n",
        "  # time.sleep(0.1) # Uncomment this line if you want to see every episode"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 999\n",
            "-78.43 | -74.84 | -75.18 | -74.59 | -73.22 | \n",
            "-78.60 | -75.15 | -74.70 | -71.49 | -71.07 | \n",
            "-80.83 | -73.45 | -70.77 | -64.85 | -61.74 | \n",
            "-78.66 | -74.20 | -69.07 | -53.74 | -43.05 | \n",
            "-78.70 | -72.13 | -63.13 | -36.13 |    @   | \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSc-PczGE4b2"
      },
      "source": [
        "#### Plotting the Optimal Policy\n",
        "\n",
        "That's right! The optimal policy is to choose the action that picks the highest expected return. In other words, you want to move the agent towards regions of higher reward.\n",
        "\n",
        "Let me create another helper function to visualise where the maximal actions are pointing..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udC_5ImwYBA1",
        "outputId": "1e96ab61-8b58-4df3-bf51-a3471bf50afe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def argmax(a):\n",
        "    return max(range(len(a)), key=lambda x: a[x])\n",
        "\n",
        "def next_best_value_2d(env, agent):\n",
        "    res = \"\"\n",
        "    for y in reversed(range(env.height)):\n",
        "      for x in range(env.width):\n",
        "        if env.goal.x == x and env.goal.y == y:\n",
        "          res += \"@\"\n",
        "        else:\n",
        "          # Find the action that has the highest value\n",
        "          loc = argmax([agent.action_value(Point(x,y), d) for d in env.action_space])\n",
        "          res += f'{env.action_space[loc].value}'\n",
        "        res += \" | \"\n",
        "      res += \"\\n\"\n",
        "    return res\n",
        "\n",
        "print(next_best_value_2d(env, agent))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "⬆ | ⬇ | ⬅ | ⬇ | ⬇ | \n",
            "⬅ | ⬆ | ⬇ | ⬇ | ⬇ | \n",
            "⮕ | ⮕ | ⮕ | ⬇ | ⬇ | \n",
            "⮕ | ⮕ | ⮕ | ⬇ | ⬇ | \n",
            "⮕ | ⮕ | ⮕ | ⮕ | @ | \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsd37WzCFcSB"
      },
      "source": [
        "And there you have it. A policy. The above image spells out what the agent should do in each state. It should move towards regions of higher value. You can see (in general) that the arrows are all pointing towards the goal, as if by magic.\n",
        "\n",
        "For the arrows that are not, that is a more interesting story. The problem is that the agent is still entirely random at this point. It's stumbling around until it reachest the goal. The agent started in the top left, so on average, it takes a _lot_ of stumbling to find the goal.\n",
        "\n",
        "Therefore, for the points at the top left, furthest away from the goal, the agent will probably take many more random steps before it reachest the goal. In essence, it _doesn't matter which way the agent goes_. It will still take a long time to get there.\n",
        "\n",
        "Subsequent Monte Carlo algorithms fix this by updating the action value function every episode and using this knowledge to choose the action. So latter iterations of the agent are far more guided and intelligent.\n",
        "\n",
        "#### One Final Run\n",
        "\n",
        "To wrap this up, let me run the whole thing one more time. I will plot the state value function and the policy for all iteration steps. Watch how it changes over time. Add a sleep to slow it down to see it changing on each step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiCODK9C3qR9",
        "outputId": "b5189349-19fe-4d6a-883b-adee08557618",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = SimpleGridWorld() # Instantiate the environment\n",
        "generator = MonteCarloGeneration(env=env) # Instantiate the trajectory generator\n",
        "agent = MonteCarloExperiment(generator=generator)\n",
        "for i in range(1000):\n",
        "  clear_output(wait=True)\n",
        "  agent.run_episode()\n",
        "  print(f\"Iteration: {i}\")\n",
        "  print(state_value_2d(env, agent))\n",
        "  print(next_best_value_2d(env, agent), flush=True)\n",
        "  # time.sleep(0.1) # Uncomment this line if you want to see the iterations"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 999\n",
            "-83.81 | -82.24 | -81.58 | -81.81 | -79.21 | \n",
            "-87.66 | -84.94 | -80.75 | -79.07 | -77.20 | \n",
            "-86.06 | -82.90 | -74.89 | -69.01 | -65.00 | \n",
            "-84.64 | -81.34 | -67.66 | -56.00 | -43.77 | \n",
            "-76.32 | -78.16 | -63.25 | -39.12 |    @   | \n",
            "\n",
            "⬅ | ⬆ | ⬆ | ⬇ | ⬇ | \n",
            "⮕ | ⬇ | ⬇ | ⬇ | ⬇ | \n",
            "⮕ | ⮕ | ⬇ | ⬇ | ⬇ | \n",
            "⬇ | ⮕ | ⮕ | ⬇ | ⬇ | \n",
            "⬇ | ⮕ | ⮕ | ⮕ | @ | \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R5gh9xjG1ZR"
      },
      "source": [
        "## Summary and Next Steps\n",
        "\n",
        "I appreciate that this might be the first time that you have encountered Monte Carlo (MC) techniques or RL. So I have intentionally made this notebook as simple and free of libraries as possible, to gain experience at the coal-face.\n",
        "\n",
        "This obviously means that the actual algorithm isn't that intelligent. For example, MC techniques usually go through two phases, policy evaluation, where full trajectories are captured, then policy improvement, where a new policy is derived. This helps stabilise and speed up learning, beacuse you are learning on every episode.\n",
        "\n",
        "Subsequent algorithms then take this idea further and update the policy on every _step_, which is called _bootstrapping_ which supercharges the RL algorithm to produce _temporal difference_ algorithms (see Chapter 3 in the book).\n",
        "\n",
        "But I'm getting ahead of myself. I really do encourage you to play around with this code and tinker with the results. Here are some things that you can try:\n",
        "\n",
        "- Increase or decrease the size of the grid.\n",
        "- Add other terminating states\n",
        "- Change the reward to a different value\n",
        "- Change the reward to produce 0 reward per step, and a positive reward for the terminating state\n",
        "- Add a terminating state with a massive negative reward, to simulate a cliff\n",
        "- Add a hole in the middle\n",
        "- Add a wall\n",
        "- See if you can add the code to use the policy derived by the agent\n",
        "\n",
        "And of course, there's far more details in the book. Please [consider buying it](https://rl-book.com/?utm_source=winder&utm_medium=notebook&utm_campaign=rl) if you want to learn more. Good luck!\n",
        "\n",
        "[Source](https://rl-book.com/learn/mdp/code_driven_intro/?utm_source=winder&utm_medium=notebook&utm_campaign=rl)."
      ]
    }
  ]
}